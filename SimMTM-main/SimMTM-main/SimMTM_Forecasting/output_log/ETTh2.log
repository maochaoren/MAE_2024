nohup: ignoring input
Args in experiment:
Namespace(task_name='pretrain', is_training=1, model_id='ETTh2', model='SimMTM', data='ETTh2', root_path='./dataset/ETT-small/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=48, pred_len=96, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=7, dec_in=7, c_out=7, d_model=8, n_heads=4, e_layers=2, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='mov_avg', window_size=97, st_sep=7.0, top_k_fft=25, lpf=50, patching_s=1, patching_t=0, patch_len_s=48, patch_len_t=16, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=8, is_early_stop=1, patience=5, learning_rate=0.0007, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=3, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
Use GPU: cuda:0
number of model params 7272739
>>>>>>>start pre_training : pretrain_SimMTM_ETTh2_M_isdec1_decmetmov_avg_win97_sep7.0_topk25_sl384_ll48_pl96_dm8_df64_nh4_el2_dl1_fc1_dp0.1_hdp0.1_ep40_bs8_lr0.0007_lm3_pn3_mr0.5_tp0.2>>>>>>>>>>>>>>>>>>>>>>>>>>
<class 'numpy.ndarray'>
(8640, 7)
train 8161 1020
<class 'numpy.ndarray'>
(3264, 7)
val 2785 348
Epoch: 0, Lr: 0.0006978, Time: 445.04s | Train Loss: 8.2307/20.2551/0.2212Val Loss: 5.7218/18.2487/0.0818
Validation loss decreased (5.7218 --> 5.7218).  Saving model epoch0 ...
Epoch: 1, Lr: 0.0006925, Time: 451.28s | Train Loss: 5.3521/19.1604/0.1389Val Loss: 4.5640/17.9320/0.0719
Validation loss decreased (5.7218 --> 4.5640).  Saving model epoch1 ...
Epoch: 2, Lr: 0.0006850, Time: 456.87s | Train Loss: 4.5258/18.9321/0.1174Val Loss: 4.0763/17.7693/0.0629
Validation loss decreased (4.5640 --> 4.0763).  Saving model epoch2 ...
Epoch: 3, Lr: 0.0006755, Time: 452.24s | Train Loss: 4.1406/18.8001/0.1078Val Loss: 3.8290/17.6604/0.0590
Validation loss decreased (4.0763 --> 3.8290).  Saving model epoch3 ...
Epoch: 4, Lr: 0.0006640, Time: 440.67s | Train Loss: 3.9371/18.7347/0.1011Val Loss: 3.7002/17.5920/0.0567
Validation loss decreased (3.8290 --> 3.7002).  Saving model epoch4 ...
Epoch: 5, Lr: 0.0006505, Time: 442.59s | Train Loss: 3.8263/18.6784/0.0967Val Loss: 3.6333/17.5859/0.0539
Validation loss decreased (3.7002 --> 3.6333).  Saving model epoch5 ...
Epoch: 6, Lr: 0.0006353, Time: 524.55s | Train Loss: 3.7671/18.6254/0.0931Val Loss: 3.6076/17.5242/0.0546
Validation loss decreased (3.6333 --> 3.6076).  Saving model epoch6 ...
Epoch: 7, Lr: 0.0006182, Time: 700.25s | Train Loss: 3.7380/18.5885/0.0900Val Loss: 3.5920/17.5196/0.0513
Validation loss decreased (3.6076 --> 3.5920).  Saving model epoch7 ...
Epoch: 8, Lr: 0.0005996, Time: 410.31s | Train Loss: 3.7243/18.5547/0.0876Val Loss: 3.6055/17.6684/0.0533
Early stopping count: 1
Epoch: 9, Lr: 0.0005794, Time: 565.00s | Train Loss: 3.7187/18.5409/0.0859Val Loss: 3.5844/17.5032/0.0478
Validation loss decreased (3.5920 --> 3.5844).  Saving model epoch9 ...
Epoch: 10, Lr: 0.0005578, Time: 515.51s | Train Loss: 3.7115/18.4953/0.0837Val Loss: 3.5793/17.4825/0.0471
Validation loss decreased (3.5844 --> 3.5793).  Saving model epoch10 ...
Epoch: 11, Lr: 0.0005349, Time: 440.90s | Train Loss: 3.7055/18.4505/0.0820Val Loss: 3.5723/17.3806/0.0460
Validation loss decreased (3.5793 --> 3.5723).  Saving model epoch11 ...
Epoch: 12, Lr: 0.0005110, Time: 448.42s | Train Loss: 3.7014/18.4221/0.0808Val Loss: 3.5729/17.3786/0.0464
Early stopping count: 1
Epoch: 13, Lr: 0.0004860, Time: 457.91s | Train Loss: 3.6979/18.3961/0.0798Val Loss: 3.5685/17.3701/0.0455
Validation loss decreased (3.5723 --> 3.5685).  Saving model epoch13 ...
Epoch: 14, Lr: 0.0004602, Time: 447.29s | Train Loss: 3.6937/18.3644/0.0787Val Loss: 3.5592/17.2818/0.0440
Validation loss decreased (3.5685 --> 3.5592).  Saving model epoch14 ...
Epoch: 15, Lr: 0.0004337, Time: 445.51s | Train Loss: 3.6902/18.3355/0.0777Val Loss: 3.5572/17.2830/0.0426
Validation loss decreased (3.5592 --> 3.5572).  Saving model epoch15 ...
Epoch: 16, Lr: 0.0004068, Time: 452.74s | Train Loss: 3.6866/18.3151/0.0766Val Loss: 3.5606/17.2811/0.0443
Early stopping count: 1
Epoch: 17, Lr: 0.0003795, Time: 450.17s | Train Loss: 3.6852/18.2976/0.0764Val Loss: 3.5508/17.2272/0.0410
Validation loss decreased (3.5572 --> 3.5508).  Saving model epoch17 ...
Epoch: 18, Lr: 0.0003520, Time: 447.24s | Train Loss: 3.6811/18.2639/0.0754Val Loss: 3.5536/17.2341/0.0419
Early stopping count: 1
Epoch: 19, Lr: 0.0003245, Time: 457.13s | Train Loss: 3.6785/18.2489/0.0746Val Loss: 3.5468/17.1883/0.0410
Validation loss decreased (3.5508 --> 3.5468).  Saving model epoch19 ...
Epoch: 20, Lr: 0.0002972, Time: 477.09s | Train Loss: 3.6774/18.2354/0.0744Val Loss: 3.5496/17.2075/0.0414
Early stopping count: 1
Epoch: 21, Lr: 0.0002703, Time: 473.81s | Train Loss: 3.6747/18.2059/0.0739Val Loss: 3.5434/17.1906/0.0398
Validation loss decreased (3.5468 --> 3.5434).  Saving model epoch21 ...
Epoch: 22, Lr: 0.0002438, Time: 543.68s | Train Loss: 3.6726/18.1966/0.0732Val Loss: 3.5375/17.1370/0.0387
Validation loss decreased (3.5434 --> 3.5375).  Saving model epoch22 ...
Epoch: 23, Lr: 0.0002180, Time: 537.54s | Train Loss: 3.6714/18.1856/0.0729Val Loss: 3.5328/17.1008/0.0378
Validation loss decreased (3.5375 --> 3.5328).  Saving model epoch23 ...
Epoch: 24, Lr: 0.0001930, Time: 473.83s | Train Loss: 3.6687/18.1591/0.0723Val Loss: 3.5333/17.0819/0.0385
Early stopping count: 1
Epoch: 25, Lr: 0.0001690, Time: 464.34s | Train Loss: 3.6677/18.1547/0.0720Val Loss: 3.5333/17.0959/0.0378
Early stopping count: 2
Epoch: 26, Lr: 0.0001462, Time: 682.67s | Train Loss: 3.6659/18.1468/0.0714Val Loss: 3.5308/17.0842/0.0372
Validation loss decreased (3.5328 --> 3.5308).  Saving model epoch26 ...
Epoch: 27, Lr: 0.0001245, Time: 450.95s | Train Loss: 3.6643/18.1289/0.0711Val Loss: 3.5290/17.0681/0.0372
Validation loss decreased (3.5308 --> 3.5290).  Saving model epoch27 ...
Epoch: 28, Lr: 0.0001043, Time: 372.65s | Train Loss: 3.6626/18.1172/0.0707Val Loss: 3.5320/17.1019/0.0379
Early stopping count: 1
Epoch: 29, Lr: 0.0000857, Time: 355.60s | Train Loss: 3.6625/18.1160/0.0706Val Loss: 3.5310/17.0997/0.0374
Early stopping count: 2
Epoch: 30, Lr: 0.0000686, Time: 402.58s | Train Loss: 3.6610/18.1011/0.0703Val Loss: 3.5289/17.0488/0.0376
Validation loss decreased (3.5290 --> 3.5289).  Saving model epoch30 ...
Epoch: 31, Lr: 0.0000533, Time: 392.98s | Train Loss: 3.6596/18.0959/0.0699Val Loss: 3.5259/17.0376/0.0368
Validation loss decreased (3.5289 --> 3.5259).  Saving model epoch31 ...
Epoch: 32, Lr: 0.0000398, Time: 428.72s | Train Loss: 3.6598/18.0928/0.0700Val Loss: 3.5281/17.0728/0.0370
Early stopping count: 1
Epoch: 33, Lr: 0.0000282, Time: 402.12s | Train Loss: 3.6587/18.0874/0.0697Val Loss: 3.5254/17.0468/0.0365
Validation loss decreased (3.5259 --> 3.5254).  Saving model epoch33 ...
Epoch: 34, Lr: 0.0000186, Time: 353.67s | Train Loss: 3.6576/18.0754/0.0695Val Loss: 3.5258/17.0396/0.0369
Early stopping count: 1
Epoch: 35, Lr: 0.0000110, Time: 316.17s | Train Loss: 3.6578/18.0796/0.0695Val Loss: 3.5239/17.0266/0.0363
Validation loss decreased (3.5254 --> 3.5239).  Saving model epoch35 ...
Epoch: 36, Lr: 0.0000055, Time: 317.89s | Train Loss: 3.6574/18.0762/0.0694Val Loss: 3.5234/17.0154/0.0364
Validation loss decreased (3.5239 --> 3.5234).  Saving model epoch36 ...
Epoch: 37, Lr: 0.0000019, Time: 368.85s | Train Loss: 3.6569/18.0735/0.0693Val Loss: 3.5252/17.0391/0.0367
Early stopping count: 1
Epoch: 38, Lr: 0.0000003, Time: 317.03s | Train Loss: 3.6567/18.0708/0.0692Val Loss: 3.5271/17.0613/0.0370
Early stopping count: 2
Epoch: 39, Lr: 0.0000000, Time: 336.32s | Train Loss: 3.6561/18.0697/0.0690Val Loss: 3.5253/17.0535/0.0364
Early stopping count: 3
bash: /data1/home/xurui//MAE_2024/SimMTM-main/SimMTM-main/SimMTM_Forecasting/scripts/finetune/ETT_script/ETTh2.sh: Permission denied

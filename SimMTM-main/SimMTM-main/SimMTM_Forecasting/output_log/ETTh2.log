nohup: ignoring input
Args in experiment:
Namespace(task_name='pretrain', is_training=1, model_id='ETTh2', model='SimMTM', data='ETTh2', root_path='./dataset/ETT-small/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=48, pred_len=96, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=7, dec_in=7, c_out=7, d_model=8, n_heads=4, e_layers=2, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='fft', window_size=97, st_sep=7.0, top_k_fft=25, lpf=50, patching_s=0, patching_t=0, patch_len_s=192, patch_len_t=8, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=8, is_early_stop=1, patience=4, learning_rate=0.001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=2, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
decomp_method:fft,st_sep:7.0,lpf:50,s_patching:0,s_patch_len:192,t_patching:0,t_patch_len:8
Use GPU: cuda:0
number of model params 12007651
>>>>>>>start pre_training : pretrain_SimMTM_ETTh2_M_isdec1_decmetfft_win97_sep7.0_topk25_sl384_ll48_pl96_dm8_df64_nh4_el2_dl1_fc1_dp0.1_hdp0.1_ep40_bs8_lr0.001_lm3_pn2_mr0.5_tp0.2>>>>>>>>>>>>>>>>>>>>>>>>>>
<class 'numpy.ndarray'>
(8640, 7)
train 8161 1020
<class 'numpy.ndarray'>
(3264, 7)
val 2785 348
Epoch: 0, Lr: 0.0009969, Time: 128.82s | Train Loss: 4.2030/8.3825/0.2657Val Loss: 3.1877/7.7124/0.0745
Validation loss decreased (3.1877 --> 3.1877).  Saving model epoch0 ...
Epoch: 1, Lr: 0.0009893, Time: 128.49s | Train Loss: 3.1526/7.8401/0.1147Val Loss: 2.9182/7.5213/0.0559
Validation loss decreased (3.1877 --> 2.9182).  Saving model epoch1 ...
Epoch: 2, Lr: 0.0009786, Time: 127.02s | Train Loss: 2.9976/7.6452/0.0999Val Loss: 2.8464/7.3549/0.0509
Validation loss decreased (2.9182 --> 2.8464).  Saving model epoch2 ...
Epoch: 3, Lr: 0.0009650, Time: 126.34s | Train Loss: 2.9576/7.5378/0.0951Val Loss: 2.8193/7.1727/0.0491
Validation loss decreased (2.8464 --> 2.8193).  Saving model epoch3 ...
Epoch: 4, Lr: 0.0009485, Time: 127.61s | Train Loss: 2.9389/7.4363/0.0925Val Loss: 2.8162/7.1352/0.0480
Validation loss decreased (2.8193 --> 2.8162).  Saving model epoch4 ...
Epoch: 5, Lr: 0.0009293, Time: 127.84s | Train Loss: 2.9250/7.3511/0.0904Val Loss: 2.8064/7.0859/0.0483
Validation loss decreased (2.8162 --> 2.8064).  Saving model epoch5 ...
Epoch: 6, Lr: 0.0009075, Time: 129.52s | Train Loss: 2.9145/7.2785/0.0893Val Loss: 2.7987/7.0157/0.0473
Validation loss decreased (2.8064 --> 2.7987).  Saving model epoch6 ...
Epoch: 7, Lr: 0.0008832, Time: 129.23s | Train Loss: 2.9028/7.1950/0.0882Val Loss: 2.7874/6.9559/0.0467
Validation loss decreased (2.7987 --> 2.7874).  Saving model epoch7 ...
Epoch: 8, Lr: 0.0008566, Time: 128.76s | Train Loss: 2.8927/7.1294/0.0870Val Loss: 2.7752/6.8907/0.0459
Validation loss decreased (2.7874 --> 2.7752).  Saving model epoch8 ...
Epoch: 9, Lr: 0.0008277, Time: 128.94s | Train Loss: 2.8831/7.0640/0.0860Val Loss: 2.7681/6.8511/0.0455
Validation loss decreased (2.7752 --> 2.7681).  Saving model epoch9 ...
Epoch: 10, Lr: 0.0007969, Time: 133.43s | Train Loss: 2.8756/7.0083/0.0855Val Loss: 2.7734/6.8512/0.0465
Early stopping count: 1
Epoch: 11, Lr: 0.0007642, Time: 134.51s | Train Loss: 2.8688/6.9650/0.0847Val Loss: 2.7586/6.7660/0.0458
Validation loss decreased (2.7681 --> 2.7586).  Saving model epoch11 ...
Epoch: 12, Lr: 0.0007299, Time: 134.47s | Train Loss: 2.8624/6.9206/0.0842Val Loss: 2.7534/6.7165/0.0452
Validation loss decreased (2.7586 --> 2.7534).  Saving model epoch12 ...
Epoch: 13, Lr: 0.0006943, Time: 135.52s | Train Loss: 2.8565/6.8799/0.0836Val Loss: 2.7576/6.7300/0.0459
Early stopping count: 1
Epoch: 14, Lr: 0.0006574, Time: 135.03s | Train Loss: 2.8515/6.8474/0.0831Val Loss: 2.7412/6.6503/0.0447
Validation loss decreased (2.7534 --> 2.7412).  Saving model epoch14 ...
Epoch: 15, Lr: 0.0006196, Time: 135.95s | Train Loss: 2.8460/6.8146/0.0824Val Loss: 2.7418/6.6408/0.0442
Early stopping count: 1
Epoch: 16, Lr: 0.0005811, Time: 135.38s | Train Loss: 2.8429/6.7929/0.0822Val Loss: 2.7477/6.6906/0.0452
Early stopping count: 2
Epoch: 17, Lr: 0.0005421, Time: 135.01s | Train Loss: 2.8375/6.7550/0.0818Val Loss: 2.7337/6.5759/0.0437
Validation loss decreased (2.7412 --> 2.7337).  Saving model epoch17 ...
Epoch: 18, Lr: 0.0005029, Time: 133.12s | Train Loss: 2.8346/6.7376/0.0814Val Loss: 2.7313/6.5797/0.0436
Validation loss decreased (2.7337 --> 2.7313).  Saving model epoch18 ...
Epoch: 19, Lr: 0.0004636, Time: 132.24s | Train Loss: 2.8315/6.7164/0.0812Val Loss: 2.7239/6.5207/0.0442
Validation loss decreased (2.7313 --> 2.7239).  Saving model epoch19 ...
Epoch: 20, Lr: 0.0004246, Time: 138.99s | Train Loss: 2.8288/6.7033/0.0807Val Loss: 2.7250/6.5466/0.0433
Early stopping count: 1
Epoch: 21, Lr: 0.0003861, Time: 137.22s | Train Loss: 2.8262/6.6876/0.0803Val Loss: 2.7271/6.5560/0.0433
Early stopping count: 2
Epoch: 22, Lr: 0.0003483, Time: 136.63s | Train Loss: 2.8238/6.6699/0.0802Val Loss: 2.7228/6.5406/0.0428
Validation loss decreased (2.7239 --> 2.7228).  Saving model epoch22 ...
Epoch: 23, Lr: 0.0003114, Time: 136.02s | Train Loss: 2.8212/6.6589/0.0797Val Loss: 2.7230/6.5445/0.0430
Early stopping count: 1
Epoch: 24, Lr: 0.0002758, Time: 133.42s | Train Loss: 2.8195/6.6476/0.0795Val Loss: 2.7223/6.5327/0.0429
Validation loss decreased (2.7228 --> 2.7223).  Saving model epoch24 ...
Epoch: 25, Lr: 0.0002415, Time: 137.05s | Train Loss: 2.8168/6.6286/0.0793Val Loss: 2.7160/6.4849/0.0428
Validation loss decreased (2.7223 --> 2.7160).  Saving model epoch25 ...
Epoch: 26, Lr: 0.0002088, Time: 141.56s | Train Loss: 2.8152/6.6220/0.0790Val Loss: 2.7176/6.4901/0.0430
Early stopping count: 1
Epoch: 27, Lr: 0.0001779, Time: 135.85s | Train Loss: 2.8139/6.6136/0.0789Val Loss: 2.7174/6.4865/0.0430
Early stopping count: 2
Epoch: 28, Lr: 0.0001491, Time: 135.57s | Train Loss: 2.8121/6.6040/0.0786Val Loss: 2.7135/6.4656/0.0427
Validation loss decreased (2.7160 --> 2.7135).  Saving model epoch28 ...
Epoch: 29, Lr: 0.0001224, Time: 135.63s | Train Loss: 2.8111/6.5989/0.0784Val Loss: 2.7172/6.5041/0.0425
Early stopping count: 1
Epoch: 30, Lr: 0.0000980, Time: 132.93s | Train Loss: 2.8092/6.5880/0.0782Val Loss: 2.7125/6.4584/0.0424
Validation loss decreased (2.7135 --> 2.7125).  Saving model epoch30 ...
Epoch: 31, Lr: 0.0000761, Time: 134.95s | Train Loss: 2.8082/6.5846/0.0780Val Loss: 2.7118/6.4627/0.0426
Validation loss decreased (2.7125 --> 2.7118).  Saving model epoch31 ...
Epoch: 32, Lr: 0.0000569, Time: 132.38s | Train Loss: 2.8082/6.5843/0.0779Val Loss: 2.7143/6.4825/0.0424
Early stopping count: 1
Epoch: 33, Lr: 0.0000403, Time: 135.40s | Train Loss: 2.8060/6.5693/0.0778Val Loss: 2.7109/6.4610/0.0422
Validation loss decreased (2.7118 --> 2.7109).  Saving model epoch33 ...
Epoch: 34, Lr: 0.0000266, Time: 136.87s | Train Loss: 2.8068/6.5806/0.0775Val Loss: 2.7101/6.4495/0.0424
Validation loss decreased (2.7109 --> 2.7101).  Saving model epoch34 ...
Epoch: 35, Lr: 0.0000157, Time: 138.10s | Train Loss: 2.8061/6.5729/0.0776Val Loss: 2.7106/6.4569/0.0423
Early stopping count: 1
Epoch: 36, Lr: 0.0000078, Time: 135.83s | Train Loss: 2.8060/6.5737/0.0775Val Loss: 2.7124/6.4716/0.0423
Early stopping count: 2
Epoch: 37, Lr: 0.0000027, Time: 135.63s | Train Loss: 2.8048/6.5654/0.0775Val Loss: 2.7118/6.4670/0.0423
Early stopping count: 3
Epoch: 38, Lr: 0.0000004, Time: 136.27s | Train Loss: 2.8051/6.5679/0.0775Val Loss: 2.7128/6.4752/0.0423
Early stopping count: 4
Early stopping
Args in experiment:
Namespace(task_name='finetune', is_training=1, model_id='ETTh2', model='SimMTM', data='ETTh2', root_path='./dataset/ETT-small/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=48, pred_len=96, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=7, dec_in=7, c_out=7, d_model=8, n_heads=4, e_layers=2, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='fft', window_size=97, st_sep=7.0, top_k_fft=25, lpf=50, patching_s=0, patching_t=0, patch_len_s=24, patch_len_t=4, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=32, is_early_stop=1, patience=2, learning_rate=0.0001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=3, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
./outputs/pretrain_checkpoints/ETTh2/ckpt_best.pth
Use GPU: cuda:0
Loading ckpt: ./outputs/pretrain_checkpoints/ETTh2/ckpt_best.pth
weights from ./outputs/pretrain_checkpoints/ETTh2/ckpt_best.pth successfully transferred!

number of model params 594464
>>>>>>>start training : finetune_SimMTM_ETTh2_M_isdec1_decmetfft_win97_sep7.0_topk25_sl384_ll48_pl96_dm8_df64_nh4_el2_dl1_fc1_dp0.1_hdp0.1_ep40_bs32_lr0.0001>>>>>>>>>>>>>>>>>>>>>>>>>>
<class 'numpy.ndarray'>
(8640, 7)
train 8161 255
<class 'numpy.ndarray'>
(3264, 7)
val 2785 87
<class 'numpy.ndarray'>
(3264, 7)
test 2785 2785
Epoch: 1, Steps: 255, Time: 40.70s | Train Loss: 0.5859933 Vali Loss: 0.3413333 Test Loss: 0.3746859
Validation loss decreased (inf --> 0.341333).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 255, Time: 39.40s | Train Loss: 0.4691879 Vali Loss: 0.2348345 Test Loss: 0.2920917
Validation loss decreased (0.341333 --> 0.234835).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 255, Time: 39.86s | Train Loss: 0.4043846 Vali Loss: 0.2313497 Test Loss: 0.2907073
Validation loss decreased (0.234835 --> 0.231350).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 255, Time: 39.67s | Train Loss: 0.3913812 Vali Loss: 0.2317544 Test Loss: 0.2913505
EarlyStopping counter: 1 out of 2
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 255, Time: 40.14s | Train Loss: 0.3864051 Vali Loss: 0.2316337 Test Loss: 0.2909547
EarlyStopping counter: 2 out of 2
Early stopping
>>>>>>>testing : finetune_SimMTM_ETTh2_M_isdec1_decmetfft_win97_sep7.0_topk25_sl384_ll48_pl96_dm8_df64_nh4_el2_dl1_fc1_dp0.1_hdp0.1_ep40_bs32_lr0.0001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
<class 'numpy.ndarray'>
(3264, 7)
test 2785 2785
384->96, mse:0.291, mae:0.346
Args in experiment:
Namespace(task_name='finetune', is_training=1, model_id='ETTh2', model='SimMTM', data='ETTh2', root_path='./dataset/ETT-small/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=48, pred_len=192, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=7, dec_in=7, c_out=7, d_model=8, n_heads=4, e_layers=2, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='fft', window_size=97, st_sep=7.0, top_k_fft=25, lpf=50, patching_s=0, patching_t=0, patch_len_s=24, patch_len_t=4, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=32, is_early_stop=1, patience=2, learning_rate=0.0001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=3, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
./outputs/pretrain_checkpoints/ETTh2/ckpt_best.pth
Use GPU: cuda:0
Loading ckpt: ./outputs/pretrain_checkpoints/ETTh2/ckpt_best.pth
weights from ./outputs/pretrain_checkpoints/ETTh2/ckpt_best.pth successfully transferred!

number of model params 1184480
>>>>>>>start training : finetune_SimMTM_ETTh2_M_isdec1_decmetfft_win97_sep7.0_topk25_sl384_ll48_pl192_dm8_df64_nh4_el2_dl1_fc1_dp0.1_hdp0.1_ep40_bs32_lr0.0001>>>>>>>>>>>>>>>>>>>>>>>>>>
<class 'numpy.ndarray'>
(8640, 7)
train 8065 252
<class 'numpy.ndarray'>
(3264, 7)
val 2689 84
<class 'numpy.ndarray'>
(3264, 7)
test 2689 2689
Epoch: 1, Steps: 252, Time: 38.77s | Train Loss: 0.6442332 Vali Loss: 0.3959085 Test Loss: 0.3900348
Validation loss decreased (inf --> 0.395909).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 252, Time: 38.38s | Train Loss: 0.5391144 Vali Loss: 0.2948287 Test Loss: 0.3536494
Validation loss decreased (0.395909 --> 0.294829).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 252, Time: 40.11s | Train Loss: 0.4827452 Vali Loss: 0.2978388 Test Loss: 0.3524660
EarlyStopping counter: 1 out of 2
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 252, Time: 39.41s | Train Loss: 0.4717739 Vali Loss: 0.3000050 Test Loss: 0.3552731
EarlyStopping counter: 2 out of 2
Early stopping
>>>>>>>testing : finetune_SimMTM_ETTh2_M_isdec1_decmetfft_win97_sep7.0_topk25_sl384_ll48_pl192_dm8_df64_nh4_el2_dl1_fc1_dp0.1_hdp0.1_ep40_bs32_lr0.0001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
<class 'numpy.ndarray'>
(3264, 7)
test 2689 2689
384->192, mse:0.354, mae:0.383
Args in experiment:
Namespace(task_name='finetune', is_training=1, model_id='ETTh2', model='SimMTM', data='ETTh2', root_path='./dataset/ETT-small/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=48, pred_len=336, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=7, dec_in=7, c_out=7, d_model=8, n_heads=4, e_layers=2, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='fft', window_size=97, st_sep=7.0, top_k_fft=25, lpf=50, patching_s=0, patching_t=0, patch_len_s=24, patch_len_t=4, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=32, is_early_stop=1, patience=2, learning_rate=0.0001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=3, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
./outputs/pretrain_checkpoints/ETTh2/ckpt_best.pth
Use GPU: cuda:0
Loading ckpt: ./outputs/pretrain_checkpoints/ETTh2/ckpt_best.pth
weights from ./outputs/pretrain_checkpoints/ETTh2/ckpt_best.pth successfully transferred!

number of model params 2069504
>>>>>>>start training : finetune_SimMTM_ETTh2_M_isdec1_decmetfft_win97_sep7.0_topk25_sl384_ll48_pl336_dm8_df64_nh4_el2_dl1_fc1_dp0.1_hdp0.1_ep40_bs32_lr0.0001>>>>>>>>>>>>>>>>>>>>>>>>>>
<class 'numpy.ndarray'>
(8640, 7)
train 7921 247
<class 'numpy.ndarray'>
(3264, 7)
val 2545 79
<class 'numpy.ndarray'>
(3264, 7)
test 2545 2545
Epoch: 1, Steps: 247, Time: 37.73s | Train Loss: 0.7200972 Vali Loss: 0.4829870 Test Loss: 0.3839802
Validation loss decreased (inf --> 0.482987).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 247, Time: 37.75s | Train Loss: 0.6224608 Vali Loss: 0.3855912 Test Loss: 0.3676859
Validation loss decreased (0.482987 --> 0.385591).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 247, Time: 38.36s | Train Loss: 0.5696907 Vali Loss: 0.3925332 Test Loss: 0.3748391
EarlyStopping counter: 1 out of 2
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 247, Time: 38.46s | Train Loss: 0.5578715 Vali Loss: 0.3930137 Test Loss: 0.3745976
EarlyStopping counter: 2 out of 2
Early stopping
>>>>>>>testing : finetune_SimMTM_ETTh2_M_isdec1_decmetfft_win97_sep7.0_topk25_sl384_ll48_pl336_dm8_df64_nh4_el2_dl1_fc1_dp0.1_hdp0.1_ep40_bs32_lr0.0001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
<class 'numpy.ndarray'>
(3264, 7)
test 2545 2545
384->336, mse:0.368, mae:0.401
Args in experiment:
Namespace(task_name='finetune', is_training=1, model_id='ETTh2', model='SimMTM', data='ETTh2', root_path='./dataset/ETT-small/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=48, pred_len=720, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=7, dec_in=7, c_out=7, d_model=8, n_heads=4, e_layers=2, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='fft', window_size=97, st_sep=7.0, top_k_fft=25, lpf=50, patching_s=0, patching_t=0, patch_len_s=24, patch_len_t=4, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=32, is_early_stop=1, patience=2, learning_rate=0.0001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=3, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
./outputs/pretrain_checkpoints/ETTh2/ckpt_best.pth
Use GPU: cuda:0
Loading ckpt: ./outputs/pretrain_checkpoints/ETTh2/ckpt_best.pth
weights from ./outputs/pretrain_checkpoints/ETTh2/ckpt_best.pth successfully transferred!

number of model params 4429568
>>>>>>>start training : finetune_SimMTM_ETTh2_M_isdec1_decmetfft_win97_sep7.0_topk25_sl384_ll48_pl720_dm8_df64_nh4_el2_dl1_fc1_dp0.1_hdp0.1_ep40_bs32_lr0.0001>>>>>>>>>>>>>>>>>>>>>>>>>>
<class 'numpy.ndarray'>
(8640, 7)
train 7537 235
<class 'numpy.ndarray'>
(3264, 7)
val 2161 67
<class 'numpy.ndarray'>
(3264, 7)
test 2161 2161
Epoch: 1, Steps: 235, Time: 34.56s | Train Loss: 0.8852683 Vali Loss: 0.7414401 Test Loss: 0.4258249
Validation loss decreased (inf --> 0.741440).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 235, Time: 34.02s | Train Loss: 0.8024860 Vali Loss: 0.6408303 Test Loss: 0.3953380
Validation loss decreased (0.741440 --> 0.640830).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 235, Time: 34.40s | Train Loss: 0.7603306 Vali Loss: 0.6448199 Test Loss: 0.3990384
EarlyStopping counter: 1 out of 2
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 235, Time: 34.22s | Train Loss: 0.7506093 Vali Loss: 0.6483549 Test Loss: 0.3990380
EarlyStopping counter: 2 out of 2
Early stopping
>>>>>>>testing : finetune_SimMTM_ETTh2_M_isdec1_decmetfft_win97_sep7.0_topk25_sl384_ll48_pl720_dm8_df64_nh4_el2_dl1_fc1_dp0.1_hdp0.1_ep40_bs32_lr0.0001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
<class 'numpy.ndarray'>
(3264, 7)
test 2161 2161
384->720, mse:0.395, mae:0.431

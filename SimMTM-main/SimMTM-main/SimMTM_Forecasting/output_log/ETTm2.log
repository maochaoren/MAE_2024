nohup: ignoring input
Args in experiment:
Namespace(task_name='pretrain', is_training=1, model_id='ETTm2', model='SimMTM', data='ETTm2', root_path='./dataset/ETT-small/', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=48, pred_len=96, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=7, dec_in=7, c_out=7, d_model=8, n_heads=8, e_layers=2, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='fft', window_size=97, st_sep=3.0, top_k_fft=25, lpf=30, patching_s=0, patching_t=0, patch_len_s=192, patch_len_t=48, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=8, is_early_stop=1, patience=3, learning_rate=0.001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=2, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
decomp_method:fft,st_sep:3.0,lpf:30,s_patching:0,s_patch_len:192,t_patching:0,t_patch_len:48
Use GPU: cuda:0
number of model params 12005203
>>>>>>>start pre_training : pretrain_SimMTM_ETTm2_M_isdec1_decmetfft_win97_sep3.0_topk25_sl384_ll48_pl96_dm8_df16_nh8_el2_dl1_fc1_dp0.1_hdp0.1_ep40_bs8_lr0.001_lm3_pn2_mr0.5_tp0.2>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34081 4260
val 11425 1428
Epoch: 0, Lr: 0.0009969, Time: 785.50s | Train Loss: 3.2305/7.9091/0.0915Val Loss: 2.7409/6.9861/0.0334
Validation loss decreased (2.7409 --> 2.7409).  Saving model epoch0 ...
Epoch: 1, Lr: 0.0009893, Time: 823.24s | Train Loss: 2.8323/7.3333/0.0530Val Loss: 2.6659/6.4505/0.0301
Validation loss decreased (2.7409 --> 2.6659).  Saving model epoch1 ...
Epoch: 2, Lr: 0.0009786, Time: 845.85s | Train Loss: 2.7953/7.0736/0.0504Val Loss: 2.6413/6.2946/0.0287
Validation loss decreased (2.6659 --> 2.6413).  Saving model epoch2 ...
Epoch: 3, Lr: 0.0009650, Time: 848.55s | Train Loss: 2.7746/6.9313/0.0489Val Loss: 2.6225/6.1788/0.0280
Validation loss decreased (2.6413 --> 2.6225).  Saving model epoch3 ...
Epoch: 4, Lr: 0.0009485, Time: 852.59s | Train Loss: 2.7632/6.8533/0.0482Val Loss: 2.6200/6.1429/0.0287
Validation loss decreased (2.6225 --> 2.6200).  Saving model epoch4 ...
Epoch: 5, Lr: 0.0009293, Time: 850.06s | Train Loss: 2.7548/6.7948/0.0477Val Loss: 2.6136/6.1144/0.0282
Validation loss decreased (2.6200 --> 2.6136).  Saving model epoch5 ...
Epoch: 6, Lr: 0.0009075, Time: 833.30s | Train Loss: 2.7477/6.7459/0.0472Val Loss: 2.6117/6.1042/0.0276
Validation loss decreased (2.6136 --> 2.6117).  Saving model epoch6 ...
Epoch: 7, Lr: 0.0008832, Time: 796.90s | Train Loss: 2.7430/6.7125/0.0470Val Loss: 2.6061/6.0803/0.0273
Validation loss decreased (2.6117 --> 2.6061).  Saving model epoch7 ...
Epoch: 8, Lr: 0.0008566, Time: 785.75s | Train Loss: 2.7387/6.6834/0.0468Val Loss: 2.6068/6.0621/0.0272
Early stopping count: 1
Epoch: 9, Lr: 0.0008277, Time: 781.77s | Train Loss: 2.7349/6.6584/0.0465Val Loss: 2.6048/6.0753/0.0274
Validation loss decreased (2.6061 --> 2.6048).  Saving model epoch9 ...
Epoch: 10, Lr: 0.0007969, Time: 779.62s | Train Loss: 2.7324/6.6404/0.0464Val Loss: 2.6054/6.0567/0.0275
Early stopping count: 1
Epoch: 11, Lr: 0.0007642, Time: 777.61s | Train Loss: 2.7296/6.6221/0.0462Val Loss: 2.6130/6.0571/0.0298
Early stopping count: 2
Epoch: 12, Lr: 0.0007299, Time: 777.48s | Train Loss: 2.7278/6.6093/0.0461Val Loss: 2.6028/6.0591/0.0273
Validation loss decreased (2.6048 --> 2.6028).  Saving model epoch12 ...
Epoch: 13, Lr: 0.0006943, Time: 772.05s | Train Loss: 2.7259/6.5964/0.0460Val Loss: 2.6019/6.0388/0.0273
Validation loss decreased (2.6028 --> 2.6019).  Saving model epoch13 ...
Epoch: 14, Lr: 0.0006574, Time: 777.32s | Train Loss: 2.7243/6.5857/0.0459Val Loss: 2.5972/6.0376/0.0270
Validation loss decreased (2.6019 --> 2.5972).  Saving model epoch14 ...
Epoch: 15, Lr: 0.0006196, Time: 779.74s | Train Loss: 2.7227/6.5749/0.0458Val Loss: 2.5939/6.0086/0.0270
Validation loss decreased (2.5972 --> 2.5939).  Saving model epoch15 ...
Epoch: 16, Lr: 0.0005811, Time: 778.15s | Train Loss: 2.7209/6.5618/0.0458Val Loss: 2.6014/6.0486/0.0273
Early stopping count: 1
Epoch: 17, Lr: 0.0005421, Time: 758.42s | Train Loss: 2.7198/6.5569/0.0456Val Loss: 2.5920/5.9860/0.0269
Validation loss decreased (2.5939 --> 2.5920).  Saving model epoch17 ...
Epoch: 18, Lr: 0.0005029, Time: 755.05s | Train Loss: 2.7180/6.5452/0.0455Val Loss: 2.5926/5.9913/0.0270
Early stopping count: 1
Epoch: 19, Lr: 0.0004636, Time: 793.41s | Train Loss: 2.7172/6.5399/0.0454Val Loss: 2.5912/5.9828/0.0270
Validation loss decreased (2.5920 --> 2.5912).  Saving model epoch19 ...
Epoch: 20, Lr: 0.0004246, Time: 792.42s | Train Loss: 2.7155/6.5292/0.0453Val Loss: 2.5928/5.9725/0.0270
Early stopping count: 1
Epoch: 21, Lr: 0.0003861, Time: 914.80s | Train Loss: 2.7146/6.5242/0.0452Val Loss: 2.5976/6.0304/0.0271
Early stopping count: 2
Epoch: 22, Lr: 0.0003483, Time: 765.20s | Train Loss: 2.7137/6.5192/0.0451Val Loss: 2.5913/5.9842/0.0271
Early stopping count: 3
Early stopping
Args in experiment:
Namespace(task_name='finetune', is_training=1, model_id='ETTm2', model='SimMTM', data='ETTm2', root_path='./dataset/ETT-small/', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=48, pred_len=96, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=7, dec_in=7, c_out=7, d_model=8, n_heads=8, e_layers=3, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.0, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='fft', window_size=97, st_sep=3.0, top_k_fft=25, lpf=30, patching_s=0, patching_t=0, patch_len_s=24, patch_len_t=48, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=32, is_early_stop=1, patience=3, learning_rate=0.0001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=3, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
./outputs/pretrain_checkpoints/ETTm2/ckpt_best.pth
Use GPU: cuda:0
Loading ckpt: ./outputs/pretrain_checkpoints/ETTm2/ckpt_best.pth
weights from ./outputs/pretrain_checkpoints/ETTm2/ckpt_best.pth successfully transferred!

number of model params 592016
>>>>>>>start training : finetune_SimMTM_ETTm2_M_isdec1_decmetfft_win97_sep3.0_topk25_sl384_ll48_pl96_dm8_df16_nh8_el3_dl1_fc1_dp0.0_hdp0.1_ep40_bs32_lr0.0001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34081 1065
val 11425 357
test 11425 11425
Epoch: 1, Steps: 1065, Time: 219.10s | Train Loss: 0.3899478 Vali Loss: 0.1971987 Test Loss: 0.2627124
Validation loss decreased (inf --> 0.197199).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 1065, Time: 219.10s | Train Loss: 0.2525178 Vali Loss: 0.1251390 Test Loss: 0.1737176
Validation loss decreased (0.197199 --> 0.125139).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 1065, Time: 217.10s | Train Loss: 0.2226043 Vali Loss: 0.1255172 Test Loss: 0.1736537
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 1065, Time: 216.09s | Train Loss: 0.2179749 Vali Loss: 0.1260166 Test Loss: 0.1740515
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 1065, Time: 216.72s | Train Loss: 0.2159683 Vali Loss: 0.1250640 Test Loss: 0.1741341
Validation loss decreased (0.125139 --> 0.125064).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 1065, Time: 216.13s | Train Loss: 0.2150956 Vali Loss: 0.1253472 Test Loss: 0.1741455
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 1065, Time: 215.73s | Train Loss: 0.2146554 Vali Loss: 0.1254081 Test Loss: 0.1740860
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 1065, Time: 216.08s | Train Loss: 0.2144573 Vali Loss: 0.1254124 Test Loss: 0.1740906
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : finetune_SimMTM_ETTm2_M_isdec1_decmetfft_win97_sep3.0_topk25_sl384_ll48_pl96_dm8_df16_nh8_el3_dl1_fc1_dp0.0_hdp0.1_ep40_bs32_lr0.0001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11425 11425
384->96, mse:0.174, mae:0.266
Args in experiment:
Namespace(task_name='finetune', is_training=1, model_id='ETTm2', model='SimMTM', data='ETTm2', root_path='./dataset/ETT-small/', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=48, pred_len=192, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=7, dec_in=7, c_out=7, d_model=8, n_heads=8, e_layers=3, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.0, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='fft', window_size=97, st_sep=3.0, top_k_fft=25, lpf=30, patching_s=0, patching_t=0, patch_len_s=24, patch_len_t=48, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=32, is_early_stop=1, patience=3, learning_rate=0.0001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=3, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
./outputs/pretrain_checkpoints/ETTm2/ckpt_best.pth
Use GPU: cuda:0
Loading ckpt: ./outputs/pretrain_checkpoints/ETTm2/ckpt_best.pth
weights from ./outputs/pretrain_checkpoints/ETTm2/ckpt_best.pth successfully transferred!

number of model params 1182032
>>>>>>>start training : finetune_SimMTM_ETTm2_M_isdec1_decmetfft_win97_sep3.0_topk25_sl384_ll48_pl192_dm8_df16_nh8_el3_dl1_fc1_dp0.0_hdp0.1_ep40_bs32_lr0.0001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33985 1062
val 11329 354
test 11329 11329
Epoch: 1, Steps: 1062, Time: 215.60s | Train Loss: 0.4401634 Vali Loss: 0.2233121 Test Loss: 0.2932673
Validation loss decreased (inf --> 0.223312).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 1062, Time: 215.05s | Train Loss: 0.3190904 Vali Loss: 0.1682412 Test Loss: 0.2287532
Validation loss decreased (0.223312 --> 0.168241).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 1062, Time: 242.42s | Train Loss: 0.2875104 Vali Loss: 0.1705061 Test Loss: 0.2302790
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 1062, Time: 240.34s | Train Loss: 0.2823184 Vali Loss: 0.1731062 Test Loss: 0.2320658
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 1062, Time: 242.87s | Train Loss: 0.2801725 Vali Loss: 0.1725118 Test Loss: 0.2318733
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : finetune_SimMTM_ETTm2_M_isdec1_decmetfft_win97_sep3.0_topk25_sl384_ll48_pl192_dm8_df16_nh8_el3_dl1_fc1_dp0.0_hdp0.1_ep40_bs32_lr0.0001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329 11329
384->192, mse:0.229, mae:0.302
Args in experiment:
Namespace(task_name='finetune', is_training=1, model_id='ETTm2', model='SimMTM', data='ETTm2', root_path='./dataset/ETT-small/', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=48, pred_len=336, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=7, dec_in=7, c_out=7, d_model=8, n_heads=8, e_layers=3, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.0, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='fft', window_size=97, st_sep=3.0, top_k_fft=25, lpf=30, patching_s=0, patching_t=0, patch_len_s=24, patch_len_t=48, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=32, is_early_stop=1, patience=3, learning_rate=0.0001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=3, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
./outputs/pretrain_checkpoints/ETTm2/ckpt_best.pth
Use GPU: cuda:0
Loading ckpt: ./outputs/pretrain_checkpoints/ETTm2/ckpt_best.pth
weights from ./outputs/pretrain_checkpoints/ETTm2/ckpt_best.pth successfully transferred!

number of model params 2067056
>>>>>>>start training : finetune_SimMTM_ETTm2_M_isdec1_decmetfft_win97_sep3.0_topk25_sl384_ll48_pl336_dm8_df16_nh8_el3_dl1_fc1_dp0.0_hdp0.1_ep40_bs32_lr0.0001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33841 1057
val 11185 349
test 11185 11185
Epoch: 1, Steps: 1057, Time: 298.08s | Train Loss: 0.4951456 Vali Loss: 0.2533908 Test Loss: 0.3300138
Validation loss decreased (inf --> 0.253391).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 1057, Time: 273.73s | Train Loss: 0.3914501 Vali Loss: 0.2134541 Test Loss: 0.2760990
Validation loss decreased (0.253391 --> 0.213454).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 1057, Time: 260.36s | Train Loss: 0.3577188 Vali Loss: 0.2140511 Test Loss: 0.2774994
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 1057, Time: 237.01s | Train Loss: 0.3516027 Vali Loss: 0.2190627 Test Loss: 0.2793566
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 1057, Time: 219.19s | Train Loss: 0.3488771 Vali Loss: 0.2179893 Test Loss: 0.2789085
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : finetune_SimMTM_ETTm2_M_isdec1_decmetfft_win97_sep3.0_topk25_sl384_ll48_pl336_dm8_df16_nh8_el3_dl1_fc1_dp0.0_hdp0.1_ep40_bs32_lr0.0001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185 11185
384->336, mse:0.276, mae:0.335
Args in experiment:
Namespace(task_name='finetune', is_training=1, model_id='ETTm2', model='SimMTM', data='ETTm2', root_path='./dataset/ETT-small/', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=48, pred_len=720, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=7, dec_in=7, c_out=7, d_model=8, n_heads=8, e_layers=3, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.0, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='fft', window_size=97, st_sep=3.0, top_k_fft=25, lpf=30, patching_s=0, patching_t=0, patch_len_s=24, patch_len_t=48, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=32, is_early_stop=1, patience=3, learning_rate=0.0001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=3, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
./outputs/pretrain_checkpoints/ETTm2/ckpt_best.pth
Use GPU: cuda:0
Loading ckpt: ./outputs/pretrain_checkpoints/ETTm2/ckpt_best.pth
weights from ./outputs/pretrain_checkpoints/ETTm2/ckpt_best.pth successfully transferred!

number of model params 4427120
>>>>>>>start training : finetune_SimMTM_ETTm2_M_isdec1_decmetfft_win97_sep3.0_topk25_sl384_ll48_pl720_dm8_df16_nh8_el3_dl1_fc1_dp0.0_hdp0.1_ep40_bs32_lr0.0001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33457 1045
val 10801 337
test 10801 10801
Epoch: 1, Steps: 1045, Time: 211.65s | Train Loss: 0.5935017 Vali Loss: 0.3079951 Test Loss: 0.4110264
Validation loss decreased (inf --> 0.307995).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 1045, Time: 211.64s | Train Loss: 0.5111272 Vali Loss: 0.2806479 Test Loss: 0.3592647
Validation loss decreased (0.307995 --> 0.280648).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 1045, Time: 216.21s | Train Loss: 0.4826797 Vali Loss: 0.2852099 Test Loss: 0.3594039
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 1045, Time: 220.27s | Train Loss: 0.4756587 Vali Loss: 0.2831993 Test Loss: 0.3605008
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 1045, Time: 219.57s | Train Loss: 0.4727164 Vali Loss: 0.2838677 Test Loss: 0.3598405
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : finetune_SimMTM_ETTm2_M_isdec1_decmetfft_win97_sep3.0_topk25_sl384_ll48_pl720_dm8_df16_nh8_el3_dl1_fc1_dp0.0_hdp0.1_ep40_bs32_lr0.0001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801 10801
384->720, mse:0.359, mae:0.386

nohup: ignoring input
Args in experiment:
Namespace(task_name='pretrain', is_training=1, model_id='ETTm1', model='SimMTM', data='ETTm1', root_path='./dataset/ETT-small/', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=48, pred_len=96, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=7, dec_in=7, c_out=7, d_model=8, n_heads=8, e_layers=1, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='fft', window_size=97, st_sep=3.0, top_k_fft=25, lpf=50, patching_s=1, patching_t=0, patch_len_s=192, patch_len_t=48, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=8, is_early_stop=1, patience=3, learning_rate=0.001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=2, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
decomp_method:fft,st_sep:3.0,lpf:50,s_patching:1,s_patch_len:192,t_patching:0,t_patch_len:48
Use GPU: cuda:0
number of model params 8417251
>>>>>>>start pre_training : pretrain_SimMTM_ETTm1_M_isdec1_decmetfft_win97_sep3.0_topk25_sl384_ll48_pl96_dm8_df64_nh8_el1_dl1_fc1_dp0.1_hdp0.1_ep40_bs8_lr0.001_lm3_pn2_mr0.5_tp0.2>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34081 4260
val 11425 1428
Epoch: 0, Lr: 0.0009969, Time: 760.81s | Train Loss: 3.4158/8.6023/0.1163Val Loss: 2.9031/7.5777/0.0686
Validation loss decreased (2.9031 --> 2.9031).  Saving model epoch0 ...
Epoch: 1, Lr: 0.0009893, Time: 760.67s | Train Loss: 2.9768/8.0235/0.0811Val Loss: 2.8730/7.3813/0.0653
Validation loss decreased (2.9031 --> 2.8730).  Saving model epoch1 ...
Epoch: 2, Lr: 0.0009786, Time: 752.69s | Train Loss: 2.9450/7.8039/0.0772Val Loss: 2.8353/7.2102/0.0569
Validation loss decreased (2.8730 --> 2.8353).  Saving model epoch2 ...
Epoch: 3, Lr: 0.0009650, Time: 749.04s | Train Loss: 2.9276/7.6851/0.0752Val Loss: 2.8273/7.1793/0.0560
Validation loss decreased (2.8353 --> 2.8273).  Saving model epoch3 ...
Epoch: 4, Lr: 0.0009485, Time: 755.98s | Train Loss: 2.9171/7.6117/0.0741Val Loss: 2.8247/7.1062/0.0581
Validation loss decreased (2.8273 --> 2.8247).  Saving model epoch4 ...
Epoch: 5, Lr: 0.0009293, Time: 745.04s | Train Loss: 2.9097/7.5633/0.0732Val Loss: 2.8322/7.1183/0.0607
Early stopping count: 1
Epoch: 6, Lr: 0.0009075, Time: 744.08s | Train Loss: 2.9042/7.5286/0.0724Val Loss: 2.8244/7.1002/0.0589
Validation loss decreased (2.8247 --> 2.8244).  Saving model epoch6 ...
Epoch: 7, Lr: 0.0008832, Time: 743.54s | Train Loss: 2.8998/7.5007/0.0719Val Loss: 2.8205/7.0756/0.0584
Validation loss decreased (2.8244 --> 2.8205).  Saving model epoch7 ...
Epoch: 8, Lr: 0.0008566, Time: 744.10s | Train Loss: 2.8964/7.4796/0.0714Val Loss: 2.8230/7.0641/0.0599
Early stopping count: 1
Epoch: 9, Lr: 0.0008277, Time: 1127.92s | Train Loss: 2.8933/7.4606/0.0710Val Loss: 2.8184/7.0247/0.0596
Validation loss decreased (2.8205 --> 2.8184).  Saving model epoch9 ...
Epoch: 10, Lr: 0.0007969, Time: 1978.59s | Train Loss: 2.8909/7.4460/0.0707Val Loss: 2.8319/7.0715/0.0633
Early stopping count: 1
Epoch: 11, Lr: 0.0007642, Time: 762.58s | Train Loss: 2.8887/7.4323/0.0704Val Loss: 2.8275/7.0065/0.0645
Early stopping count: 2
Epoch: 12, Lr: 0.0007299, Time: 757.65s | Train Loss: 2.8868/7.4210/0.0701Val Loss: 2.8278/7.0348/0.0634
Early stopping count: 3
Early stopping
Args in experiment:
Namespace(task_name='finetune', is_training=1, model_id='ETTm1', model='SimMTM', data='ETTm1', root_path='./dataset/ETT-small/', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=24, pred_len=96, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=7, dec_in=7, c_out=7, d_model=8, n_heads=8, e_layers=2, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.0, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='fft', window_size=97, st_sep=3.0, top_k_fft=25, lpf=50, patching_s=0, patching_t=0, patch_len_s=24, patch_len_t=48, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=32, is_early_stop=1, patience=3, learning_rate=0.0001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=3, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
./outputs/pretrain_checkpoints/ETTm1/ckpt_best.pth
Use GPU: cuda:0
Loading ckpt: ./outputs/pretrain_checkpoints/ETTm1/ckpt_best.pth
weights from ./outputs/pretrain_checkpoints/ETTm1/ckpt_best.pth successfully transferred!

number of model params 594464
>>>>>>>start training : finetune_SimMTM_ETTm1_M_isdec1_decmetfft_win97_sep3.0_topk25_sl384_ll24_pl96_dm8_df64_nh8_el2_dl1_fc1_dp0.0_hdp0.1_ep40_bs32_lr0.0001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34081 1065
val 11425 357
test 11425 11425
Epoch: 1, Steps: 1065, Time: 218.29s | Train Loss: 0.4504608 Vali Loss: 0.6974875 Test Loss: 0.5141928
Validation loss decreased (inf --> 0.697487).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 1065, Time: 222.40s | Train Loss: 0.2873846 Vali Loss: 0.3962827 Test Loss: 0.3156386
Validation loss decreased (0.697487 --> 0.396283).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 1065, Time: 228.48s | Train Loss: 0.2632204 Vali Loss: 0.3877658 Test Loss: 0.3111723
Validation loss decreased (0.396283 --> 0.387766).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 1065, Time: 458.14s | Train Loss: 0.2597167 Vali Loss: 0.3859625 Test Loss: 0.3104067
Validation loss decreased (0.387766 --> 0.385962).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 1065, Time: 473.05s | Train Loss: 0.2582626 Vali Loss: 0.3842605 Test Loss: 0.3089032
Validation loss decreased (0.385962 --> 0.384261).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 1065, Time: 464.77s | Train Loss: 0.2576420 Vali Loss: 0.3837315 Test Loss: 0.3082926
Validation loss decreased (0.384261 --> 0.383731).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 1065, Time: 465.05s | Train Loss: 0.2572018 Vali Loss: 0.3834286 Test Loss: 0.3083588
Validation loss decreased (0.383731 --> 0.383429).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 1065, Time: 462.22s | Train Loss: 0.2570534 Vali Loss: 0.3834404 Test Loss: 0.3080665
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 1065, Time: 308.46s | Train Loss: 0.2568764 Vali Loss: 0.3835091 Test Loss: 0.3082997
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 1065, Time: 215.60s | Train Loss: 0.2570584 Vali Loss: 0.3835567 Test Loss: 0.3083676
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : finetune_SimMTM_ETTm1_M_isdec1_decmetfft_win97_sep3.0_topk25_sl384_ll24_pl96_dm8_df64_nh8_el2_dl1_fc1_dp0.0_hdp0.1_ep40_bs32_lr0.0001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11425 11425
384->96, mse:0.308, mae:0.357
Args in experiment:
Namespace(task_name='finetune', is_training=1, model_id='ETTm1', model='SimMTM', data='ETTm1', root_path='./dataset/ETT-small/', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=24, pred_len=192, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=7, dec_in=7, c_out=7, d_model=8, n_heads=8, e_layers=2, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.0, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='fft', window_size=97, st_sep=3.0, top_k_fft=25, lpf=50, patching_s=0, patching_t=0, patch_len_s=24, patch_len_t=48, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=32, is_early_stop=1, patience=3, learning_rate=0.0001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=3, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
./outputs/pretrain_checkpoints/ETTm1/ckpt_best.pth
Use GPU: cuda:0
Loading ckpt: ./outputs/pretrain_checkpoints/ETTm1/ckpt_best.pth
weights from ./outputs/pretrain_checkpoints/ETTm1/ckpt_best.pth successfully transferred!

number of model params 1184480
>>>>>>>start training : finetune_SimMTM_ETTm1_M_isdec1_decmetfft_win97_sep3.0_topk25_sl384_ll24_pl192_dm8_df64_nh8_el2_dl1_fc1_dp0.0_hdp0.1_ep40_bs32_lr0.0001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33985 1062
val 11329 354
test 11329 11329
Epoch: 1, Steps: 1062, Time: 215.65s | Train Loss: 0.4701007 Vali Loss: 0.7801290 Test Loss: 0.5251623
Validation loss decreased (inf --> 0.780129).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 1062, Time: 214.47s | Train Loss: 0.3225528 Vali Loss: 0.5086384 Test Loss: 0.3439315
Validation loss decreased (0.780129 --> 0.508638).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 1062, Time: 213.83s | Train Loss: 0.3021557 Vali Loss: 0.5063963 Test Loss: 0.3436407
Validation loss decreased (0.508638 --> 0.506396).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 1062, Time: 214.28s | Train Loss: 0.2988476 Vali Loss: 0.5026169 Test Loss: 0.3402814
Validation loss decreased (0.506396 --> 0.502617).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 1062, Time: 213.28s | Train Loss: 0.2975003 Vali Loss: 0.5016130 Test Loss: 0.3401975
Validation loss decreased (0.502617 --> 0.501613).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 1062, Time: 213.59s | Train Loss: 0.2968556 Vali Loss: 0.5015926 Test Loss: 0.3412964
Validation loss decreased (0.501613 --> 0.501593).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 1062, Time: 212.97s | Train Loss: 0.2964440 Vali Loss: 0.5015336 Test Loss: 0.3405032
Validation loss decreased (0.501593 --> 0.501534).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 1062, Time: 213.68s | Train Loss: 0.2963533 Vali Loss: 0.5012820 Test Loss: 0.3404708
Validation loss decreased (0.501534 --> 0.501282).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 1062, Time: 213.21s | Train Loss: 0.2963054 Vali Loss: 0.5012970 Test Loss: 0.3405748
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 1062, Time: 213.47s | Train Loss: 0.2961797 Vali Loss: 0.5011364 Test Loss: 0.3405365
Validation loss decreased (0.501282 --> 0.501136).  Saving model ...
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 1062, Time: 214.20s | Train Loss: 0.2962010 Vali Loss: 0.5012118 Test Loss: 0.3405413
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 1062, Time: 213.90s | Train Loss: 0.2961721 Vali Loss: 0.5012116 Test Loss: 0.3405376
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.8828125e-08
Epoch: 13, Steps: 1062, Time: 213.35s | Train Loss: 0.2961562 Vali Loss: 0.5011736 Test Loss: 0.3405362
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : finetune_SimMTM_ETTm1_M_isdec1_decmetfft_win97_sep3.0_topk25_sl384_ll24_pl192_dm8_df64_nh8_el2_dl1_fc1_dp0.0_hdp0.1_ep40_bs32_lr0.0001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329 11329
384->192, mse:0.341, mae:0.376
Args in experiment:
Namespace(task_name='finetune', is_training=1, model_id='ETTm1', model='SimMTM', data='ETTm1', root_path='./dataset/ETT-small/', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=24, pred_len=336, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=7, dec_in=7, c_out=7, d_model=8, n_heads=8, e_layers=2, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.0, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='fft', window_size=97, st_sep=3.0, top_k_fft=25, lpf=50, patching_s=0, patching_t=0, patch_len_s=24, patch_len_t=48, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=32, is_early_stop=1, patience=3, learning_rate=0.0001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=3, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
./outputs/pretrain_checkpoints/ETTm1/ckpt_best.pth
Use GPU: cuda:0
Loading ckpt: ./outputs/pretrain_checkpoints/ETTm1/ckpt_best.pth
weights from ./outputs/pretrain_checkpoints/ETTm1/ckpt_best.pth successfully transferred!

number of model params 2069504
>>>>>>>start training : finetune_SimMTM_ETTm1_M_isdec1_decmetfft_win97_sep3.0_topk25_sl384_ll24_pl336_dm8_df64_nh8_el2_dl1_fc1_dp0.0_hdp0.1_ep40_bs32_lr0.0001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33841 1057
val 11185 349
test 11185 11185
Epoch: 1, Steps: 1057, Time: 212.62s | Train Loss: 0.4973977 Vali Loss: 0.8970912 Test Loss: 0.5430888
Validation loss decreased (inf --> 0.897091).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 1057, Time: 211.08s | Train Loss: 0.3622126 Vali Loss: 0.6476372 Test Loss: 0.3641359
Validation loss decreased (0.897091 --> 0.647637).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 1057, Time: 212.34s | Train Loss: 0.3435894 Vali Loss: 0.6460388 Test Loss: 0.3649212
Validation loss decreased (0.647637 --> 0.646039).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 1057, Time: 211.65s | Train Loss: 0.3403043 Vali Loss: 0.6454486 Test Loss: 0.3655679
Validation loss decreased (0.646039 --> 0.645449).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 1057, Time: 211.80s | Train Loss: 0.3388767 Vali Loss: 0.6459411 Test Loss: 0.3666781
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 1057, Time: 212.22s | Train Loss: 0.3381946 Vali Loss: 0.6454232 Test Loss: 0.3669278
Validation loss decreased (0.645449 --> 0.645423).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 1057, Time: 212.21s | Train Loss: 0.3378794 Vali Loss: 0.6448544 Test Loss: 0.3666718
Validation loss decreased (0.645423 --> 0.644854).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 1057, Time: 212.02s | Train Loss: 0.3376807 Vali Loss: 0.6452616 Test Loss: 0.3667410
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 1057, Time: 211.80s | Train Loss: 0.3375391 Vali Loss: 0.6452066 Test Loss: 0.3667505
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 1057, Time: 213.25s | Train Loss: 0.3375278 Vali Loss: 0.6452522 Test Loss: 0.3667795
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : finetune_SimMTM_ETTm1_M_isdec1_decmetfft_win97_sep3.0_topk25_sl384_ll24_pl336_dm8_df64_nh8_el2_dl1_fc1_dp0.0_hdp0.1_ep40_bs32_lr0.0001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185 11185
384->336, mse:0.367, mae:0.393
Args in experiment:
Namespace(task_name='finetune', is_training=1, model_id='ETTm1', model='SimMTM', data='ETTm1', root_path='./dataset/ETT-small/', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=24, pred_len=720, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=7, dec_in=7, c_out=7, d_model=8, n_heads=8, e_layers=2, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.0, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='fft', window_size=97, st_sep=3.0, top_k_fft=25, lpf=50, patching_s=0, patching_t=0, patch_len_s=24, patch_len_t=48, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=32, is_early_stop=1, patience=3, learning_rate=0.0001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=3, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
./outputs/pretrain_checkpoints/ETTm1/ckpt_best.pth
Use GPU: cuda:0
Loading ckpt: ./outputs/pretrain_checkpoints/ETTm1/ckpt_best.pth
weights from ./outputs/pretrain_checkpoints/ETTm1/ckpt_best.pth successfully transferred!

number of model params 4429568
>>>>>>>start training : finetune_SimMTM_ETTm1_M_isdec1_decmetfft_win97_sep3.0_topk25_sl384_ll24_pl720_dm8_df64_nh8_el2_dl1_fc1_dp0.0_hdp0.1_ep40_bs32_lr0.0001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33457 1045
val 10801 337
test 10801 10801
Epoch: 1, Steps: 1045, Time: 209.86s | Train Loss: 0.5497179 Vali Loss: 1.1771692 Test Loss: 0.5769382
Validation loss decreased (inf --> 1.177169).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 1045, Time: 208.18s | Train Loss: 0.4250198 Vali Loss: 0.9478588 Test Loss: 0.4105006
Validation loss decreased (1.177169 --> 0.947859).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 1045, Time: 208.76s | Train Loss: 0.4074564 Vali Loss: 0.9497012 Test Loss: 0.4110371
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 1045, Time: 209.68s | Train Loss: 0.4042487 Vali Loss: 0.9478950 Test Loss: 0.4107002
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 1045, Time: 211.04s | Train Loss: 0.4029085 Vali Loss: 0.9471648 Test Loss: 0.4113542
Validation loss decreased (0.947859 --> 0.947165).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 1045, Time: 208.63s | Train Loss: 0.4022874 Vali Loss: 0.9486889 Test Loss: 0.4118976
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 1045, Time: 212.89s | Train Loss: 0.4019506 Vali Loss: 0.9487562 Test Loss: 0.4117540
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 1045, Time: 209.64s | Train Loss: 0.4018103 Vali Loss: 0.9486677 Test Loss: 0.4119342
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : finetune_SimMTM_ETTm1_M_isdec1_decmetfft_win97_sep3.0_topk25_sl384_ll24_pl720_dm8_df64_nh8_el2_dl1_fc1_dp0.0_hdp0.1_ep40_bs32_lr0.0001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801 10801
384->720, mse:0.411, mae:0.423

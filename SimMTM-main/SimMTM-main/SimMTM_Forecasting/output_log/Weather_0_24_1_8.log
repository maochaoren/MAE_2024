nohup: ignoring input
Args in experiment:
Namespace(task_name='finetune', is_training=1, model_id='Weather', model='SimMTM', is_finetune=0, data='Weather', root_path='./dataset/weather/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=48, pred_len=96, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=21, dec_in=21, c_out=21, d_model=8, n_heads=8, e_layers=1, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='mov_avg', window_size=97, st_sep=5.0, top_k_fft=25, lpf=50, patching_s=0, patching_t=1, patch_len_s=24, patch_len_t=8, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=16, is_early_stop=1, patience=3, learning_rate=0.0001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=3, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
None
Use GPU: cuda:0
number of model params 336488
>>>>>>>start training : finetune_SimMTM_Weather_M_isdec1_decmetmov_avg_win97_sep5.0_topk25_sl384_ll48_pl96_dm8_df64_nh8_el1_dl1_fc1_dp0.1_hdp0.1_ep40_bs16_lr0.0001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36408 2275
val 5175 323
test 10444 10444
Epoch: 1, Steps: 2275, Time: 423.84s | Train Loss: 0.6511671 Vali Loss: 0.4930438 Test Loss: 0.2167657
Validation loss decreased (inf --> 0.493044).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 2275, Time: 422.46s | Train Loss: 0.4827835 Vali Loss: 0.4264620 Test Loss: 0.1758839
Validation loss decreased (0.493044 --> 0.426462).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 2275, Time: 435.56s | Train Loss: 0.4590284 Vali Loss: 0.4164819 Test Loss: 0.1697304
Validation loss decreased (0.426462 --> 0.416482).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 2275, Time: 423.91s | Train Loss: 0.4545633 Vali Loss: 0.4112956 Test Loss: 0.1672796
Validation loss decreased (0.416482 --> 0.411296).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 2275, Time: 421.98s | Train Loss: 0.4523419 Vali Loss: 0.4103056 Test Loss: 0.1664728
Validation loss decreased (0.411296 --> 0.410306).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 2275, Time: 422.24s | Train Loss: 0.4515953 Vali Loss: 0.4096084 Test Loss: 0.1659879
Validation loss decreased (0.410306 --> 0.409608).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 2275, Time: 428.54s | Train Loss: 0.4509838 Vali Loss: 0.4100795 Test Loss: 0.1659739
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 2275, Time: 434.37s | Train Loss: 0.4508604 Vali Loss: 0.4106856 Test Loss: 0.1661070
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 2275, Time: 431.37s | Train Loss: 0.4507024 Vali Loss: 0.4096152 Test Loss: 0.1658037
EarlyStopping counter: 3 out of 3
Early stopping
Traceback (most recent call last):
  File "/data1/home/xurui/MAE_2024/SimMTM-main/SimMTM-main/SimMTM_Forecasting/run.py", line 195, in <module>
    exp.train(setting)
  File "/data1/home/xurui/MAE_2024/SimMTM-main/SimMTM-main/SimMTM_Forecasting/exp/exp_simmtm.py", line 310, in train
    self.model.load_state_dict(torch.load(best_model_path))
  File "/data1/home/xurui/.conda/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2189, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for Model:
	size mismatch for patch_embedding_t.weight: copying a param with shape torch.Size([8, 24]) from checkpoint, the shape in current model is torch.Size([8, 8]).
	size mismatch for head_t.linear.weight: copying a param with shape torch.Size([96, 128]) from checkpoint, the shape in current model is torch.Size([96, 384]).
Args in experiment:
Namespace(task_name='finetune', is_training=1, model_id='Weather', model='SimMTM', is_finetune=0, data='Weather', root_path='./dataset/weather/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=48, pred_len=192, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=21, dec_in=21, c_out=21, d_model=8, n_heads=8, e_layers=1, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='mov_avg', window_size=97, st_sep=5.0, top_k_fft=25, lpf=50, patching_s=0, patching_t=1, patch_len_s=24, patch_len_t=8, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=16, is_early_stop=1, patience=3, learning_rate=0.0001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=3, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
None
Use GPU: cuda:0
number of model params 668456
>>>>>>>start training : finetune_SimMTM_Weather_M_isdec1_decmetmov_avg_win97_sep5.0_topk25_sl384_ll48_pl192_dm8_df64_nh8_el1_dl1_fc1_dp0.1_hdp0.1_ep40_bs16_lr0.0001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36312 2269
val 5079 317
test 10348 10348
Epoch: 1, Steps: 2269, Time: 523.03s | Train Loss: 0.6837669 Vali Loss: 0.5569544 Test Loss: 0.2566908
Validation loss decreased (inf --> 0.556954).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 2269, Time: 834.98s | Train Loss: 0.5313980 Vali Loss: 0.4982081 Test Loss: 0.2189541
Validation loss decreased (0.556954 --> 0.498208).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 2269, Time: 844.57s | Train Loss: 0.5121333 Vali Loss: 0.4891472 Test Loss: 0.2153151
Validation loss decreased (0.498208 --> 0.489147).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 2269, Time: 844.11s | Train Loss: 0.5079522 Vali Loss: 0.4889146 Test Loss: 0.2146223
Validation loss decreased (0.489147 --> 0.488915).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 2269, Time: 843.30s | Train Loss: 0.5059551 Vali Loss: 0.4899898 Test Loss: 0.2147841
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 2269, Time: 842.42s | Train Loss: 0.5050122 Vali Loss: 0.4889036 Test Loss: 0.2140561
Validation loss decreased (0.488915 --> 0.488904).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 2269, Time: 842.55s | Train Loss: 0.5044193 Vali Loss: 0.4877006 Test Loss: 0.2137144
Validation loss decreased (0.488904 --> 0.487701).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 2269, Time: 841.10s | Train Loss: 0.5043772 Vali Loss: 0.4873179 Test Loss: 0.2136381
Validation loss decreased (0.487701 --> 0.487318).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 2269, Time: 843.35s | Train Loss: 0.5040919 Vali Loss: 0.4864784 Test Loss: 0.2132996
Validation loss decreased (0.487318 --> 0.486478).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 2269, Time: 841.32s | Train Loss: 0.5040745 Vali Loss: 0.4866993 Test Loss: 0.2133783
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 2269, Time: 777.98s | Train Loss: 0.5040965 Vali Loss: 0.4871108 Test Loss: 0.2133447
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.765625e-08
Epoch: 12, Steps: 2269, Time: 850.74s | Train Loss: 0.5040615 Vali Loss: 0.4871875 Test Loss: 0.2133740
EarlyStopping counter: 3 out of 3
Early stopping
Traceback (most recent call last):
  File "/data1/home/xurui/MAE_2024/SimMTM-main/SimMTM-main/SimMTM_Forecasting/run.py", line 195, in <module>
    exp.train(setting)
  File "/data1/home/xurui/MAE_2024/SimMTM-main/SimMTM-main/SimMTM_Forecasting/exp/exp_simmtm.py", line 310, in train
    self.model.load_state_dict(torch.load(best_model_path))
  File "/data1/home/xurui/.conda/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2189, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for Model:
	size mismatch for patch_embedding_t.weight: copying a param with shape torch.Size([8, 12]) from checkpoint, the shape in current model is torch.Size([8, 8]).
	size mismatch for head_t.linear.weight: copying a param with shape torch.Size([192, 256]) from checkpoint, the shape in current model is torch.Size([192, 384]).
Args in experiment:
Namespace(task_name='finetune', is_training=1, model_id='Weather', model='SimMTM', is_finetune=0, data='Weather', root_path='./dataset/weather/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=48, pred_len=336, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=21, dec_in=21, c_out=21, d_model=8, n_heads=8, e_layers=1, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='mov_avg', window_size=97, st_sep=5.0, top_k_fft=25, lpf=50, patching_s=0, patching_t=1, patch_len_s=24, patch_len_t=8, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=16, is_early_stop=1, patience=3, learning_rate=0.0001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=3, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
None
Use GPU: cuda:0
number of model params 1166408
>>>>>>>start training : finetune_SimMTM_Weather_M_isdec1_decmetmov_avg_win97_sep5.0_topk25_sl384_ll48_pl336_dm8_df64_nh8_el1_dl1_fc1_dp0.1_hdp0.1_ep40_bs16_lr0.0001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36168 2260
val 4935 308
test 10204 10204
Epoch: 1, Steps: 2260, Time: 851.51s | Train Loss: 0.7133261 Vali Loss: 0.6241400 Test Loss: 0.2939735
Validation loss decreased (inf --> 0.624140).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 2260, Time: 884.68s | Train Loss: 0.5807438 Vali Loss: 0.5671000 Test Loss: 0.2615061
Validation loss decreased (0.624140 --> 0.567100).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 2260, Time: 879.52s | Train Loss: 0.5620587 Vali Loss: 0.5672393 Test Loss: 0.2610214
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 2260, Time: 849.57s | Train Loss: 0.5587081 Vali Loss: 0.5666018 Test Loss: 0.2606622
Validation loss decreased (0.567100 --> 0.566602).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 2260, Time: 824.57s | Train Loss: 0.5569936 Vali Loss: 0.5640745 Test Loss: 0.2595825
Validation loss decreased (0.566602 --> 0.564075).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 2260, Time: 830.71s | Train Loss: 0.5561517 Vali Loss: 0.5636659 Test Loss: 0.2594090
Validation loss decreased (0.564075 --> 0.563666).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 2260, Time: 839.65s | Train Loss: 0.5558892 Vali Loss: 0.5655207 Test Loss: 0.2596932
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8, Steps: 2260, Time: 836.15s | Train Loss: 0.5556824 Vali Loss: 0.5636014 Test Loss: 0.2593353
Validation loss decreased (0.563666 --> 0.563601).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9, Steps: 2260, Time: 836.63s | Train Loss: 0.5554284 Vali Loss: 0.5638376 Test Loss: 0.2593133
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10, Steps: 2260, Time: 838.24s | Train Loss: 0.5554316 Vali Loss: 0.5641900 Test Loss: 0.2594260
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-07
Epoch: 11, Steps: 2260, Time: 784.30s | Train Loss: 0.5553565 Vali Loss: 0.5643206 Test Loss: 0.2593850
EarlyStopping counter: 3 out of 3
Early stopping
Traceback (most recent call last):
  File "/data1/home/xurui/MAE_2024/SimMTM-main/SimMTM-main/SimMTM_Forecasting/run.py", line 195, in <module>
    exp.train(setting)
  File "/data1/home/xurui/MAE_2024/SimMTM-main/SimMTM-main/SimMTM_Forecasting/exp/exp_simmtm.py", line 310, in train
    self.model.load_state_dict(torch.load(best_model_path))
  File "/data1/home/xurui/.conda/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2189, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for Model:
	size mismatch for patch_embedding_t.weight: copying a param with shape torch.Size([8, 24]) from checkpoint, the shape in current model is torch.Size([8, 8]).
	size mismatch for head_t.linear.weight: copying a param with shape torch.Size([336, 128]) from checkpoint, the shape in current model is torch.Size([336, 384]).
Args in experiment:
Namespace(task_name='finetune', is_training=1, model_id='Weather', model='SimMTM', is_finetune=0, data='Weather', root_path='./dataset/weather/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./outputs/checkpoints/', pretrain_checkpoints='./outputs/pretrain_checkpoints/', transfer_checkpoints='ckpt_best.pth', load_checkpoints=None, select_channels=1, seq_len=384, label_len=48, pred_len=720, seasonal_patterns='Monthly', top_k=5, num_kernels=3, enc_in=21, dec_in=21, c_out=21, d_model=8, n_heads=8, e_layers=1, s_e_layers=2, t_e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, fc_dropout=0, head_dropout=0.1, embed='timeF', activation='gelu', output_attention=False, individual=0, pct_start=0.3, decomp=1, decomp_method='mov_avg', window_size=97, st_sep=5.0, top_k_fft=25, lpf=50, patching_s=0, patching_t=1, patch_len_s=24, patch_len_t=8, stride=8, num_workers=5, itr=1, train_epochs=40, batch_size=16, is_early_stop=1, patience=3, learning_rate=0.0001, des='test', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', lm=3, positive_nums=3, rbtp=1, temperature=0.2, masked_rule='geometric', mask_rate=0.5)
None
Use GPU: cuda:0
number of model params 2494280
>>>>>>>start training : finetune_SimMTM_Weather_M_isdec1_decmetmov_avg_win97_sep5.0_topk25_sl384_ll48_pl720_dm8_df64_nh8_el1_dl1_fc1_dp0.1_hdp0.1_ep40_bs16_lr0.0001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35784 2236
val 4551 284
test 9820 9820
Epoch: 1, Steps: 2236, Time: 825.31s | Train Loss: 0.7893435 Vali Loss: 0.7136909 Test Loss: 0.3532800
Validation loss decreased (inf --> 0.713691).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2, Steps: 2236, Time: 834.36s | Train Loss: 0.6440615 Vali Loss: 0.6662011 Test Loss: 0.3300186
Validation loss decreased (0.713691 --> 0.666201).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3, Steps: 2236, Time: 830.02s | Train Loss: 0.6263715 Vali Loss: 0.6635742 Test Loss: 0.3293577
Validation loss decreased (0.666201 --> 0.663574).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4, Steps: 2236, Time: 827.72s | Train Loss: 0.6231090 Vali Loss: 0.6604840 Test Loss: 0.3278538
Validation loss decreased (0.663574 --> 0.660484).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5, Steps: 2236, Time: 833.40s | Train Loss: 0.6214345 Vali Loss: 0.6627132 Test Loss: 0.3286026
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6, Steps: 2236, Time: 828.56s | Train Loss: 0.6207261 Vali Loss: 0.6613182 Test Loss: 0.3281296
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.125e-06
Epoch: 7, Steps: 2236, Time: 829.58s | Train Loss: 0.6203901 Vali Loss: 0.6605108 Test Loss: 0.3281163
EarlyStopping counter: 3 out of 3
Early stopping
Traceback (most recent call last):
  File "/data1/home/xurui/MAE_2024/SimMTM-main/SimMTM-main/SimMTM_Forecasting/run.py", line 195, in <module>
    exp.train(setting)
  File "/data1/home/xurui/MAE_2024/SimMTM-main/SimMTM-main/SimMTM_Forecasting/exp/exp_simmtm.py", line 310, in train
    self.model.load_state_dict(torch.load(best_model_path))
  File "/data1/home/xurui/.conda/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2189, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for Model:
	size mismatch for patch_embedding_t.weight: copying a param with shape torch.Size([8, 24]) from checkpoint, the shape in current model is torch.Size([8, 8]).
	size mismatch for head_t.linear.weight: copying a param with shape torch.Size([720, 128]) from checkpoint, the shape in current model is torch.Size([720, 384]).
